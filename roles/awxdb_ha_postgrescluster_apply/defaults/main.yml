---
# Runner (must be a k3s server with kubeconfig)
pg_runner_host: "{{ groups['k3s_servers'][0] }}"
pg_kubeconfig_path: "/etc/rancher/k3s/k3s.yaml"

# Namespace where AWX + DB will live
pg_namespace: "awx"

# PostgresCluster identity
pg_cluster_name: "awxdb-ha"

# HA sizing
# pg_instances: 3

# Storage (Longhorn)
# PGDATA uses your HA class (3 replicas) for resilience
pg_storage_class: "longhorn-ha-any"

# For 40GB nodes: keep PGDATA modest (3 replicas -> ~15Gi raw)
# pg_pgdata_size: "5Gi"

# Backups (pgBackRest)
pg_enable_backups: true

# Keep repo on 1-replica class to avoid disk blow-up on 40G nodes
# (If you set this to longhorn-ha-any, repo_size is multiplied by 3)
pg_backrest_storage_class: "longhorn"   # change to "longhorn-ha-any" ONLY if you have disk headroom

# For 40GB nodes: keep repo modest
# pg_backrest_repo_size: "10Gi"

# Retention: small to avoid filling nodes
pg_backrest_retention_full: 1
pg_backrest_retention_diff: 2

# Optional: instance resources (empty = omit)
pg_resources: {}

# App user
pg_user_name: "awx"

# Wait behavior
pg_wait_ready: true
pg_wait_timeout: "20m"

# ----------------------------
# Workers-only scheduling
# ----------------------------

# Workers group (your 04-06). Adjust if your workers group differs.
pg_worker_nodes: "{{ groups['k3s_agents'] | default([]) }}"

# Reuse the same label scheme you're using for Longhorn
pg_worker_label_key: "node-role"
pg_worker_label_value: "worker"

# Auto-label workers before applying the PostgresCluster
pg_label_workers: true

# Enforce that Postgres instance pods + pgBackRest repoHost schedule only on workers
pg_workers_only: true